# scrapy的命令行
可以以无参数方式启动scrapy工具    
创建项目：
```
scrapy startproject myproject
//创建一个项目
cd myproject
```
控制项目：
```
scrapy genspider my domain mydomain.com
//创建一个新的spider
```
工具命令：
```
scrapy -h
//查看所有可用命令
```
scrapy共有两种命令，一种必须针对特定项目运行，另一种则是全局命令     
**全局命令：**
```
startproject 
scrapy startproject <project_name>
创建一个项目



settings 
scrapy settings [options]


runspider 
scrapy runspider <spider_file.py>
未创建项目时使用




shell
scrapy shell [url]
启动scrapyshell


fetch
scrapy fetch <url>
scrapy fetch --nolog www.baidu.com
使用spider的下载方式下载url并将其送到标准输出



view 
scrapy view url 
用浏览器打开指定url

version
scrapy version [-v]
-v参数输出平台等更多信息
```
**针对命令：**
```
crawl 
scrapy crawl <spider>
用crawl启动spider


check 
scrapy check [-l] <spider>
运行contract 检查



list 
scrapy list
列出当前可用spider

edit 
就是edit



parse 
scrapy parse <url> [options]
parse
• --spider=SPIDER ：跳过自动检测 spider 并强制使用特定的 spider
• --a NAME=VALUE ：设置 spider 的参数(可能被重复)
• --callback or -c ：spider 中用于解析返回(response)的回调函数
• --pipelines ：在 pipeline 中处理 item
• --rules or -r ：使用 CrawlSpider 规则来发现用来解析返回(response)的回调函数
• --noitems ：不显示爬取到的 item
• --nolinks ：不显示提取到的链接
• --nocolour ：避免使用 pygments 对输出着色
• --depth or -d ：指定跟进链接请求的层次数(默认：1)
• --verbose or -v ：显示每个请求的详细信息







genspider 
scrapy genspider [-t template] <name> <domain>
生成spider模板
可选的template: basic crawl csvfeed xmlfeed



deploy
scrapy deploy [ <target:project> | -l <target> | -L ]
部署到scrapyd服务


bench
scrapy bench
运行benchmark测试
```
可以自定义命令
