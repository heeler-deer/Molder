# items
爬取的主要目标就是从非结构性数据提取结构性数据     
假设我们声明了product item,则  
```
product['name']
product.get('name')
'name' in product
//查询值
product['last_updated']='today'
//设置值
product.keys()
prodect.items()
//获取dict
product2=Product(product)
//复制
dict(product)
//创建字典
class dispro(Product):
   disc=scrapy.Field(serializer=str)
   disce=scrapy.Field()
//继承

```
# spiders
## 参数
运行crawl时加-a可以传递参数
```
scrapy crawl myspider -a category=electronics
```
## 内置spider
**Spider** ,他的name必选，allowed_domains,start_urls,start_requests()等可选    
**crawlspider**,常用的spider，比上者多提供了rules属性
```
class scrapy.contrib.spiders.Rule(link_extractor,callback=None,cb_kwargs=None,follow=None,process_links=None,process_request=None)
```
link_extractor定义如何提取连接，callback接受一个response作为参数返回一个item以及request,cb_kwargs包含传给回调汉书的字典，follow指定是否跟进response提取的连接，process_links用来过滤,process_request用来过滤request
```
import scrapy
from scrapy.contrib.spiders import CrawlSpider,Rule
from scrapy.contrib.linkextractors import LinkExtractor
```
**xmlfeedspider:**分析xml     
必须设置下列属性：     
*iterator*(可选项：  
1. iternodes 高性能的正则迭代器
2. html，使用dom分析，数据量大时容易出错
3. xml,使用dom分析，同上
*itertag*不太清楚干啥的,提供标签名     
新的方法：adapt_response(response)--修改分析前的response并返回     
parse_node(response,selector)返回一个request以及item对象     
process_results(response,results)为整个spider提取的东西作最后处理，如设定item的id    
```
from scrapy import log
from scrapy.contrib.spiders import XMLFeedSpider
from myproject.items import TestItem
class MySpider(XMLFeedSpider):
name = 'example.com'
allowed_domains = ['example.com']
start_urls = ['http://www.example.com/feed.xml']
iterator = 'iternodes' # This is actually unnecessary, since it's the default value
itertag = 'item'
def parse_node(self, response, node):
log.msg('Hi, this is a <%s> node!: %s' % (self.itertag, ''.join(node.extract())))
item = TestItem()
item['id'] = node.xpath('@id').extract()
item['name'] = node.xpath('name').extract()
item['description'] = node.xpath('description').extract()
return item
```
**csvfeedspider:**按行遍历   
属性和方法：delimiter,quotechar,headers,parse_row(response,row)   
**sitemapspider:**发现爬取的url   
属性：sitemap_urls,sitemap_rules,sitemap_follow,sitemap_alternate_links,
```
from scrapy.contrib.spiders import SitemapSpider
class MySpider(SitemapSpider):第 7 章 Spiders | 78
sitemap_urls = ['http://www.example.com/robots.txt']
sitemap_rules = [
('/shop/', 'parse_shop'),
]
sitemap_follow = ['/sitemap_shops']
def parse_shop(self, response):
pass # ... scrape shop here ...
```
